{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import skew,boxcox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_targets = pd.read_csv('train_targets_scored.csv')\n",
    "train_features = train_features.drop('sig_id',axis=1)\n",
    "train_targets = train_targets.drop('sig_id',axis=1)\n",
    "\n",
    "train_features, test_features, train_scored, test_scored = train_test_split(train_features, train_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = train_features.reset_index(drop=True)\n",
    "train_targets = train_targets.reset_index(drop=True)\n",
    "train_scored = train_scored.reset_index(drop=True)\n",
    "test_scored = test_scored.reset_index(drop = True)\n",
    "\n",
    "train_features = train_features[:5000]\n",
    "test_features = test_features[:5000]\n",
    "train_scored = train_scored[:5000]\n",
    "test_scored = test_scored[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_skewness(data) :\n",
    "    columns = data.columns\n",
    "\n",
    "    # removing the skewness from the data set\n",
    "    skew_threshold = 0.5\n",
    "\n",
    "    for col in columns : \n",
    "        skewness_before = skew(data[col])\n",
    "\n",
    "        if abs(skewness_before) > skew_threshold:\n",
    "            # Apply Box-Cox transformation and find the best lambda\n",
    "            min_value = data[col].min()\n",
    "            if (min_value<=0):\n",
    "                data[col] += (-min_value+1)\n",
    "            transformed_data, lambda_best_fit = boxcox(data[col])  # Adding 1 to avoid zero values\n",
    "            data[col] = transformed_data  # Replace the original feature with the transformed data\n",
    "        \n",
    "        skewness_after = skew(data[col])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def oneHotEncoding(data):\n",
    "    # performing one hot encoding on discrete features\n",
    "    discrete_features = ['cp_dose','cp_time','cp_type']\n",
    "    data = pd.get_dummies(data,columns=discrete_features,dtype=int)\n",
    "    return data \n",
    "\n",
    "def remove_outliers(data,Y):\n",
    "    s = set()\n",
    "    columns = data.columns\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define lower and upper bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            if index in s:\n",
    "                continue\n",
    "            if (row[col]<lower_bound or row[col]>upper_bound):\n",
    "                data = data.drop(index,axis=0)\n",
    "                \n",
    "                Y = Y.drop(index,axis=0)\n",
    "                \n",
    "                s.add(index)\n",
    "    data = data.reset_index(drop = True)\n",
    "    Y = Y.reset_index(drop = True)\n",
    "    data = pd.DataFrame(data)\n",
    "    Y = pd.DataFrame(Y)\n",
    "    return data, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predicted_probabilities,true_labels):\n",
    "    # Compute cross-entropy loss\n",
    "    epsilon = 1e-15  # A small constant to prevent numerical instability (avoid log(0))\n",
    "    predicted_probabilities = np.clip(predicted_probabilities, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    cross_entropy_loss = -np.mean(true_labels * np.log(predicted_probabilities) + (1 - true_labels) * np.log(1 - predicted_probabilities))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column Progress: 100%|██████████| 206/206 [02:32<00:00,  1.35it/s]\n",
      "Column Progress: 100%|██████████| 206/206 [03:05<00:00,  1.11it/s]\n",
      "Column Progress: 100%|██████████| 206/206 [02:21<00:00,  1.45it/s]\n",
      "KFold Progress: 100%|██████████| 3/3 [07:59<00:00, 159.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.10363315428221058\n",
      "Average Validation Loss: 0.10639984720727623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kfold = 3\n",
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=42)\n",
    "\n",
    "# Random Forest parameters\n",
    "n_estimators = 100  # You can adjust this based on your needs\n",
    "max_depth = 5  # You can set a specific value if needed\n",
    "min_samples_split = 5  # You can adjust this based on your needs\n",
    "min_samples_leaf = 5 # You can adjust this based on your needs\n",
    "\n",
    "avg_training = []\n",
    "avg_validation = []\n",
    "\n",
    "print(\"Starting loop\")\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(train_features), total=kfold, desc=\"KFold Progress\"):\n",
    "    cross_val_scores = []\n",
    "    train_cross_val_score = []\n",
    "    # print(\"at index\", train_index, test_index)\n",
    "\n",
    "    for col in tqdm(train_scored.columns, total=len(train_scored.columns), desc=\"Column Progress\"):\n",
    "        # print(\"at column\", col)\n",
    "        X_train, X_test = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "        y_train, y_test = train_scored[col].iloc[train_index], train_scored[col].iloc[test_index]\n",
    "\n",
    "        X_train = oneHotEncoding(X_train)\n",
    "        X_test = oneHotEncoding(X_test)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Initialize a PCA object (you can adjust the number of components)\n",
    "        pca = PCA(n_components=20)  # Adjust as needed\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "        # Initialize the Random Forest classifier\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit the model and make predictions\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "\n",
    "        # Calculate cross-entropy loss\n",
    "        loss = cross_entropy(y_pred, y_test.values)\n",
    "        loss_train = cross_entropy(y_train_pred, y_train.values)\n",
    "\n",
    "        cross_val_scores.append(loss)\n",
    "        train_cross_val_score.append(loss_train)\n",
    "        # print(\"Exiting column\", col)\n",
    "\n",
    "    avg_training.append(np.mean(train_cross_val_score))\n",
    "    avg_validation.append(np.mean(cross_val_scores))\n",
    "    # print(\"Exiting index\", train_index, test_index)\n",
    "\n",
    "# Print or use the average training and validation scores as needed\n",
    "print(\"Average Training Loss:\", np.mean(avg_training))\n",
    "print(\"Average Validation Loss:\", np.mean(avg_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = 3\n",
    "\n",
    "# # Initialize a KFold object\n",
    "# kf = KFold(n_splits=kfold, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# penalty = [0.01,0.1,0.5,1,3,5,10,15]\n",
    "\n",
    "# # Initialize a list to store the cross-validation accuracies\n",
    "\n",
    "\n",
    "# # Perform k-fold cross-validation with PCA\n",
    "# itr = 0\n",
    "\n",
    "# pc = [4,10,15,40,60,80]\n",
    "# contamination = [0.01,0.005,0.1,0.2]\n",
    "# neighbour = [5,10,25,50,100,200,300]\n",
    "# avg_training = []\n",
    "# avg_validation = []\n",
    "\n",
    "# for i in range(len(pc)):\n",
    "#     cross_val_scores = []\n",
    "#     train_cross_val_score = []\n",
    "#     for train_index, test_index in kf.split(train_features):\n",
    "\n",
    "\n",
    "#         clf = LogisticRegression(penalty = 'l1', C = 0.5, max_iter=10000,solver='liblinear')\n",
    "\n",
    "\n",
    "#         # Split the data into training and testing sets for this fold\n",
    "#         X_train, X_test = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "#         y_train, y_test = train_scored.iloc[train_index], train_scored.iloc[test_index]\n",
    "\n",
    "        \n",
    "#         X_train = oneHotEncoding(X_train)\n",
    "#         X_test = oneHotEncoding(X_test)\n",
    "        \n",
    "#         # lof_model = LocalOutlierFactor(n_neighbors=neighbour[itr], contamination=0.01)\n",
    "\n",
    "#         # # Fit the model and identify outliers\n",
    "#         # outlier_scores = lof_model.fit_predict(X_train)\n",
    "#         # outlier_indices = X_train.index[outlier_scores == -1]\n",
    "\n",
    "#         # X_train = X_train.drop(outlier_indices)\n",
    "#         # y_train  = y_train.drop(outlier_indices)\n",
    "\n",
    "#         # X_train = X_train.reset_index(drop=True)\n",
    "#         # y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "#         scaler = StandardScaler()\n",
    "#         scaler.fit(X_train)\n",
    "#         X_train = scaler.transform(X_train)\n",
    "#         X_test = scaler.transform(X_test)\n",
    "\n",
    "#         # Initialize a PCA object (you can adjust the number of components)\n",
    "#         pca = PCA(n_components=pc[itr])\n",
    "#         pca.fit(X_train)\n",
    "#         X_train = pca.transform(X_train)\n",
    "#         X_test  = pca.transform(X_test)\n",
    "\n",
    "#         test_columns = y_train.columns\n",
    "#         loss = 0\n",
    "#         loss_train = 0\n",
    "#         for col in test_columns:\n",
    "#             if (y_train[col].unique().shape[0]==1):\n",
    "#                 y_pred = [0.001 for x in range(len(X_test))]\n",
    "#                 y_train_pred = [0.001 for x in range(len(X_train))]\n",
    "#             # Train a classifier on the transformed training data\n",
    "#             else:\n",
    "#                 clf.fit(X_train, y_train[col])\n",
    "            \n",
    "#                 y_train_pred = clf.predict_proba(X_train)\n",
    "#                 y_train_pred = [row[1] for row in y_train_pred]\n",
    "#                 # Make predictions on the transformed test data\n",
    "#                 y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "#                 # print(y_pred)\n",
    "#                 y_pred = [row[1] for row in y_pred]\n",
    "                \n",
    "#                 # print(y_pred)\n",
    "#                 # print(y_train_pred)\n",
    "            \n",
    "#             loss += cross_entropy(y_pred,y_test[col].values)\n",
    "\n",
    "#             loss_train += cross_entropy(y_train_pred,y_train[col].values)\n",
    "#         cross_val_scores.append(loss/test_columns.shape[0])\n",
    "#         train_cross_val_score.append(loss_train/test_columns.shape[0])\n",
    "\n",
    "#     itr += 1\n",
    "#     print(f\"---------------------------------------{itr}----------------------------------------\")\n",
    "#     # Calculate the mean and standard deviation of cross-validation scores\n",
    "#     print(f\"cross validation score for training : {train_cross_val_score}\")\n",
    "#     print(f\"cross validation scores : {cross_val_scores}\")\n",
    "#     mean_accuracy = np.mean(cross_val_scores)\n",
    "#     std_accuracy = np.std(cross_val_scores)\n",
    "#     print(f\"mean loss : {mean_accuracy}\")\n",
    "#     print(f\"std dev of loss : {std_accuracy}\")\n",
    "#     print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "#     avg_training.append(np.mean(train_cross_val_score))\n",
    "#     avg_validation.append(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(avg_training)\n",
    "# print(avg_validation)\n",
    "\n",
    "# plt.plot(pc,avg_training,label = 'training loss')\n",
    "# plt.plot(pc,avg_validation,label = 'validation loss')\n",
    "# plt.xlabel('logistic pc')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
